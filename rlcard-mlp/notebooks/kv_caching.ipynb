{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor \n",
    "\n",
    "\n",
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 22:58:45 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "/home/kacperwyrwal/fairseq/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n",
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/kacperwyrwal/.local/share/jupyter/runtime/kernel-v2-189797olwRLrsIhEzp.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacperwyrwal/miniconda3/envs/mlp/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class AverageSequencePooling(nn.Module):\n",
    "    def __init__(self, dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.mean(dim=self.dim)\n",
    "\n",
    "\n",
    "class TransformerEstimatorNetwork(nn.Module):\n",
    "    def __init__(self, input_dims: int, num_layers: int = 2, d_model: int = 128, nhead: int = 8, dim_feedforward: int = 32, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO (Kacper) maybe we should add batchnorm before embedding as in the original MLP?\n",
    "        # TODO (Kacper) also find out whether this embedding method with a simple linear layer is common\n",
    "        embedding = nn.Linear(input_dims, d_model, bias=True)\n",
    "        positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=5000)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, \n",
    "            activation='relu', \n",
    "            layer_norm_eps=1e-5, \n",
    "            batch_first=True, # [batch, seq, feature]\n",
    "            norm_first=False, # TODO (Kacper) check if modern version used layer norm prior to attention and feedforward or after\n",
    "            bias=True, \n",
    "        )\n",
    "        encoder = TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers,\n",
    "            norm=None, # TODO (Kacper) check if modern architectures use layer norm (I don't think so)\n",
    "            enable_nested_tensor=True,\n",
    "        )\n",
    "        pooling = AverageSequencePooling(dim=1) # 1 is the sequence dimension\n",
    "        linear = nn.Linear(d_model, self.num_actions, bias=True)\n",
    "\n",
    "        self._network = nn.Sequential(\n",
    "            embedding,\n",
    "            positional_encoding,\n",
    "            encoder,\n",
    "            pooling,\n",
    "            linear, \n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def network(self) -> TransformerEncoder:\n",
    "        return self._network\n",
    "    \n",
    "    def forward(self, s) -> Tensor:\n",
    "        return self.network(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
