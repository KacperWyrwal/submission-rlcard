{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "import torch \n",
    "import numpy as np \n",
    "from rlcard.agents.dqn_agent.memory import Memory\n",
    "from rlcard.agents.dqn_agent.typing import Transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have removed padding from SequenceMemory, since it is more appropriate down-the-line. For example, with current SinusoidalEmbedding there is no need for padding at all and hence no need for masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_pad_transition(transitions: list[Transition], length: int) -> list[Transition]:\n",
    "    \"\"\"Post-pad the sequence of transitions with the last transition to make it a fixed length\"\"\"\n",
    "    return transitions + [Transition.padding_transition(transitions[-1]) for _ in range(length - len(transitions))]\n",
    "\n",
    "\n",
    "class SequenceMemory(Memory):\n",
    "    ''' Memory for saving sequences of transitions\n",
    "    '''\n",
    "    def __init__(self, memory_size, batch_size, max_sequence_length: int):\n",
    "        ''' Initialize\n",
    "        Args:\n",
    "            memory_size (int): the size of the memroy buffer\n",
    "            max_sequence_length (int): the maximum length of the sequence\n",
    "        '''\n",
    "        super().__init__(memory_size, batch_size)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self._memory = []\n",
    "\n",
    "    @property\n",
    "    def memory(self) -> list[Transition]:\n",
    "        return self._memory\n",
    "\n",
    "    @property \n",
    "    def memory_size(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def sample(self):\n",
    "        ''' Sample a minibatch from the replay memory\n",
    "\n",
    "        Returns:\n",
    "            state_batch (list): a batch of sequences of states\n",
    "            action_batch (list): a batch of sequences of actions\n",
    "            reward_batch (list): a batch of sequences of rewards\n",
    "            next_state_batch (list): a batch of sequences of states\n",
    "            done_batch (list): a batch of sequences of dones\n",
    "        '''\n",
    "        padded_memory = post_pad_transition(self.memory, self.max_sequence_length)\n",
    "\n",
    "        # Sample a batch of starting indices of sequences and get sequences up to max_sequence_length\n",
    "        start_idx = torch.randint(0, self.memory_size, (self.batch_size, ))\n",
    "        sequences = [padded_memory[i:i+self.max_sequence_length] for i in start_idx] # [batch_size, sequence_length, 5]\n",
    "\n",
    "        # The processing below is a bit convoluted, but it's just mirroring what the SimpleMemory does.\n",
    "        def unpack_and_cat_transitions(sequence: list[Transition]):\n",
    "            return (\n",
    "                np.array([transition.state for transition in sequence]),\n",
    "                sequence[-1].action, # Should be the last action in the sequence\n",
    "                sequence[-1].reward, # TODO (Kacper) figure out if this should be the last reward or some combination of previous rewards\n",
    "                np.array([transition.next_state for transition in sequence]), # This has to be a sequence \n",
    "                sequence[-1].done, # Sequence done if the last state done\n",
    "                sequence[-1].legal_actions, # Take the last legal action\n",
    "            )        \n",
    "        sequences = map(unpack_and_cat_transitions, sequences)\n",
    "        samples = list(zip(*sequences))\n",
    "        return tuple(map(np.array, samples[:-1])) + (samples[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from rlcard.agents.dqn_agent.estimator import EstimatorNetwork\n",
    "from rlcard.agents.dqn_agent.typing import Transition\n",
    "\n",
    "\n",
    "def padding_mask(s: Tensor) -> Tensor:\n",
    "    return (s != Transition.padding_value()).all(dim=-1) # TODO we could add a test that no partial padding is present\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_sequence_length: int = 512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # From \"Attention is All You Need\"\n",
    "        # Alternate sine and cosine of different frequencies and decreasing amplitudes\n",
    "        position = torch.arange(max_sequence_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.positional_embedding = torch.zeros(max_sequence_length, 1, d_model)\n",
    "        self.positional_embedding[..., 0, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_embedding[..., 0, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        return self.positional_embedding[:x.size(0)]\n",
    "    \n",
    "\n",
    "class AverageSequencePooling(nn.Module):\n",
    "    def __init__(self, dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.mean(dim=self.dim)\n",
    "\n",
    "\n",
    "class TransformerEstimatorNetwork(EstimatorNetwork):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_actions=2, \n",
    "        state_shape=None, \n",
    "        num_layers: int = 2, \n",
    "        d_model: int = 128,\n",
    "        nhead: int = 8, \n",
    "        dim_feedforward: int = 32,\n",
    "        dropout: float = 0.1,\n",
    "        max_sequence_length: int = 512, \n",
    "    ):\n",
    "        super().__init__(num_actions=num_actions, state_shape=state_shape)\n",
    "\n",
    "        # TODO (Kacper) maybe we should add batchnorm before embedding as in the original MLP?\n",
    "        # TODO (Kacper) also find out whether this embedding method with a linear layer is common\n",
    "        self.embedding = nn.Linear(self.input_dims, d_model, bias=True)\n",
    "\n",
    "        # With sinusoidal embedding there is technically no limit on the sequence length. \n",
    "        # However, the performance does deteriorate with longer sequences. \n",
    "        # Thus, a limit is helpful both for performance, speed, and memory.\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_embedding = SinusoidalPositionalEmbedding(d_model=d_model, dropout=dropout, max_sequence_length=self.max_sequence_length)\n",
    "        self.embedding_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, \n",
    "            activation='relu', \n",
    "            layer_norm_eps=1e-5, \n",
    "            batch_first=True, # [batch, seq, feature]\n",
    "            norm_first=False, # TODO (Kacper) check if modern version used layer norm prior to attention and feedforward or after\n",
    "            bias=True, \n",
    "        )\n",
    "        self.encoder = TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers,\n",
    "            norm=None, # TODO (Kacper) check if modern architectures use layer norm (I don't think so)\n",
    "            enable_nested_tensor=True,\n",
    "        )\n",
    "        self.pooling = AverageSequencePooling(dim=1) # 1 is the sequence dimension\n",
    "        self.output_linear = nn.Linear(d_model, self.num_actions, bias=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, s: Tensor, pad: bool = True) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param s: Batch of input sequences [seq, batch, feature]\n",
    "        \"\"\"\n",
    "        s = self.embedding_dropout(self.embedding(s) + self.positional_embedding(s))\n",
    "        mask = padding_mask(s) if pad else None\n",
    "\n",
    "        s = self.encoder(s, src_key_padding_mask=mask)\n",
    "        s = self.pooling(s)\n",
    "        return self.output_linear(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = (12, )\n",
    "model = TransformerEstimatorNetwork(state_shape=state_shape)\n",
    "\n",
    "batch_size = 7\n",
    "sequence_length = 10\n",
    "input = torch.randn(batch_size, sequence_length, model.input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
