{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent, NFSPAgent, DQNAgent, CFRAgent\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pprint\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "\n",
    "env = rlcard.make('leduc-holdem', config={'seed': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' An example of training a reinforcement learning agent on the environments in RLCard\n",
    "'''\n",
    "\n",
    "def prepare_environment(args):\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "\n",
    "    # Seed numpy, torch, random\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(args.env, config={'seed': args.seed})\n",
    "\n",
    "    # Initialize the agent and use random agents as opponents\n",
    "    if args.algorithm == 'dqn':\n",
    "        from rlcard.agents import DQNAgent\n",
    "        if args.load_checkpoint_path != \"\":\n",
    "            agent = DQNAgent.from_checkpoint(checkpoint=torch.load(args.load_checkpoint_path))\n",
    "        else:\n",
    "            agent = DQNAgent(\n",
    "                num_actions=env.num_actions,\n",
    "                state_shape=env.state_shape[0],\n",
    "                mlp_layers= args.mlp_layers,\n",
    "                device=device,\n",
    "                save_path=args.log_dir,\n",
    "                save_every=args.save_every,\n",
    "                estimator_network=args.estimator_network,\n",
    "            )\n",
    "    elif args.algorithm == 'nfsp':\n",
    "        from rlcard.agents import NFSPAgent\n",
    "        if args.load_checkpoint_path != \"\":\n",
    "            agent = NFSPAgent.from_checkpoint(checkpoint=torch.load(args.load_checkpoint_path))\n",
    "        else:\n",
    "            agent = NFSPAgent(\n",
    "                num_actions=env.num_actions,\n",
    "                state_shape=env.state_shape[0],\n",
    "                hidden_layers_sizes=args.hidden_layers_sizes,\n",
    "                q_mlp_layers=[64,64],\n",
    "                device=device,\n",
    "                save_path=args.log_dir,\n",
    "                save_every=args.save_every,\n",
    "                estimator_network=args.estimator_network,\n",
    "            )\n",
    "    elif args.algorithm == 'cfr':\n",
    "        from rlcard.agents import CFRAgent\n",
    "        if args.load_checkpoint_path != \"\":\n",
    "            agent = CFRAgent.from_checkpoint(checkpoint=torch.load(args.load_checkpoint_path))\n",
    "        else:\n",
    "            agent = CFRAgent(\n",
    "                env,\n",
    "                #device=device,\n",
    "                model_path=args.log_dir,\n",
    "                #save_every=args.save_every\n",
    "            )\n",
    "    # Set agents to environment\n",
    "    agents = [agent]\n",
    "    for _ in range(1, env.num_players):\n",
    "        agents.append(RandomAgent(num_actions=env.num_actions)) # Random agents as opponents\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    return env, agent, agents\n",
    "\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    env, agent, agents = prepare_environment(args)\n",
    "\n",
    "# Start training\n",
    "    with Logger(args.log_dir) as logger:\n",
    "        for episode in range(args.num_episodes):\n",
    "\n",
    "            if args.algorithm == 'nfsp':\n",
    "                agents[0].sample_episode_policy()\n",
    "\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            if episode % args.evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    episode,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        args.num_eval_games,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, args.algorithm)\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(args.log_dir, 'model.pth')\n",
    "    torch.save(agent, save_path)\n",
    "    print('Model saved in', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
