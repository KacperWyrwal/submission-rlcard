{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import torch \n",
    "import rlcard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent, NFSPAgent, DQNAgent, CFRAgent\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pprint\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "\n",
    "env = rlcard.make('leduc-holdem', config={'seed': 0})\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DQN agent\n",
    "\n",
    "The code is derived from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py\n",
    "\n",
    "Copyright (c) 2019 Matthew Judell\n",
    "Copyright (c) 2019 DATA Lab at Texas A&M University\n",
    "Copyright (c) 2016 Denny Britz\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "\n",
    "from rlcard.utils.utils import remove_illegal\n",
    "from rlcard.agents.dqn_agent.memory import Memory\n",
    "from rlcard.agents.dqn_agent.estimator import Estimator\n",
    "\n",
    "\n",
    "class AlphaZeroAgent(object):\n",
    "    def __init__(self,\n",
    "                 replay_memory_size=20000,\n",
    "                 replay_memory_init_size=100,\n",
    "                 update_target_estimator_every=1000,\n",
    "                 discount_factor=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.1,\n",
    "                 epsilon_decay_steps=20000,\n",
    "                 batch_size=32,\n",
    "                 num_actions=2,\n",
    "                 state_shape=None,\n",
    "                 train_every=1,\n",
    "                 mlp_layers=None,\n",
    "                 learning_rate=0.00005,\n",
    "                 device=None,\n",
    "                 save_path=None,\n",
    "                 save_every=float('inf'),\n",
    "                 estimator_network: str = 'mlp',\n",
    "                 memory_sequence_length: int = 10,\n",
    "            ):\n",
    "\n",
    "        '''\n",
    "        Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "        Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            replay_memory_size (int): Size of the replay memory\n",
    "            replay_memory_init_size (int): Number of random experiences to sample when initializing\n",
    "              the reply memory.\n",
    "            update_target_estimator_every (int): Copy parameters from the Q estimator to the\n",
    "              target estimator every N steps\n",
    "            discount_factor (float): Gamma discount factor\n",
    "            epsilon_start (float): Chance to sample a random action when taking an action.\n",
    "              Epsilon is decayed over time and this is the start value\n",
    "            epsilon_end (float): The final minimum value of epsilon after decaying is done\n",
    "            epsilon_decay_steps (int): Number of steps to decay epsilon over\n",
    "            batch_size (int): Size of batches to sample from the replay memory\n",
    "            evaluate_every (int): Evaluate every N steps\n",
    "            num_actions (int): The number of the actions\n",
    "            state_space (list): The space of the state vector\n",
    "            train_every (int): Train the network every X steps.\n",
    "            mlp_layers (list): The layer number and the dimension of each layer in MLP\n",
    "            learning_rate (float): The learning rate of the DQN agent.\n",
    "            device (torch.device): whether to use the cpu or gpu\n",
    "            save_path (str): The path to save the model checkpoints\n",
    "            save_every (int): Save the model every X training steps\n",
    "        '''\n",
    "        self.use_raw = False\n",
    "        self.replay_memory_init_size = replay_memory_init_size\n",
    "        self.update_target_estimator_every = update_target_estimator_every\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.num_actions = num_actions\n",
    "        self.train_every = train_every\n",
    "\n",
    "        # Torch device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        # Total timesteps\n",
    "        self.total_t = 0\n",
    "\n",
    "        # Total training step\n",
    "        self.train_t = 0\n",
    "\n",
    "        # The epsilon decay scheduler\n",
    "        self.epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "        # Create estimators\n",
    "        # TODO (Kacper) I don't like passing memory_sequence_length to the estimator and to the memory. Maybe refactor.\n",
    "        self.pi_v_network = Estimator(\n",
    "            num_actions=num_actions, \n",
    "            learning_rate=learning_rate, \n",
    "            state_shape=state_shape,\n",
    "            mlp_layers=mlp_layers, \n",
    "            device=self.device,\n",
    "            estimator_network=estimator_network,\n",
    "            memory_sequence_length=memory_sequence_length,\n",
    "        )\n",
    "\n",
    "        # Create replay memory\n",
    "        self.memory = Memory.from_estimator_network(\n",
    "            estimator_network=estimator_network, memory_size=replay_memory_size, \n",
    "            batch_size=batch_size, max_sequence_length=memory_sequence_length,\n",
    "        )\n",
    "        \n",
    "        # Checkpoint saving parameters\n",
    "        self.save_path = save_path\n",
    "        self.save_every = save_every\n",
    "\n",
    "    def feed(self, ts):\n",
    "        ''' Store data in to replay buffer and train the agent. There are two stages.\n",
    "            In stage 1, populate the memory without training\n",
    "            In stage 2, train the agent every several timesteps\n",
    "\n",
    "        Args:\n",
    "            ts (list): a list of 5 elements that represent the transition\n",
    "        '''\n",
    "        (state, action, reward, next_state, done) = tuple(ts)\n",
    "        self.feed_memory(state['obs'], action, reward, next_state['obs'], list(next_state['legal_actions'].keys()), done)\n",
    "        self.total_t += 1\n",
    "        tmp = self.total_t - self.replay_memory_init_size\n",
    "        if tmp>=0 and tmp%self.train_every == 0:\n",
    "            self.train()\n",
    "\n",
    "    def step(self, state):\n",
    "        ''' Predict the action for genrating training data but\n",
    "            have the predictions disconnected from the computation graph\n",
    "\n",
    "        Args:\n",
    "            state (numpy.array): current state\n",
    "\n",
    "        Returns:\n",
    "            action (int): an action id\n",
    "        '''\n",
    "        q_values = self.predict(state) # NOTE <-- this is the only architecture dependent line\n",
    "        epsilon = self.epsilons[min(self.total_t, self.epsilon_decay_steps-1)]\n",
    "        legal_actions = list(state['legal_actions'].keys())\n",
    "        probs = np.ones(len(legal_actions), dtype=float) * epsilon / len(legal_actions)\n",
    "        best_action_idx = legal_actions.index(np.argmax(q_values))\n",
    "        probs[best_action_idx] += (1.0 - epsilon)\n",
    "        action_idx = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\n",
    "        return legal_actions[action_idx]\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        ''' Predict the action for evaluation purpose.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.array): current state\n",
    "\n",
    "        Returns:\n",
    "            action (int): an action id\n",
    "            info (dict): A dictionary containing information\n",
    "        '''\n",
    "        q_values = self.predict(state)\n",
    "        best_action = np.argmax(q_values)\n",
    "\n",
    "        info = {}\n",
    "        info['values'] = {state['raw_legal_actions'][i]: float(q_values[list(state['legal_actions'].keys())[i]]) for i in range(len(state['legal_actions']))}\n",
    "\n",
    "        return best_action, info\n",
    "\n",
    "    def predict(self, state):\n",
    "        ''' Predict the masked Q-values\n",
    "\n",
    "        Args:\n",
    "            state (numpy.array): current state\n",
    "\n",
    "        Returns:\n",
    "            q_values (numpy.array): a 1-d array where each entry represents a Q value\n",
    "        '''\n",
    "        \n",
    "        q_values = self.q_estimator.predict_nograd(np.expand_dims(state['obs'], 0))[0]\n",
    "        masked_q_values = -np.inf * np.ones(self.num_actions, dtype=float)\n",
    "        legal_actions = list(state['legal_actions'].keys())\n",
    "        masked_q_values[legal_actions] = q_values[legal_actions]\n",
    "\n",
    "        return masked_q_values\n",
    "\n",
    "    def train(self):\n",
    "        ''' Train the network\n",
    "\n",
    "        Returns:\n",
    "            loss (float): The loss of the current batch.\n",
    "        '''\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch, legal_actions_batch = self.memory.sample()\n",
    "\n",
    "        # Calculate best next actions using Q-network (Double DQN)\n",
    "        # NOTE (Kacper) the predict_nograd uses the next_state_batch, meaning that it should be a batch of sequences for the recurrent models\n",
    "        policy_next, value_next = self.pi_v_network.predict_nograd(next_state_batch)\n",
    "        # Perform gradient descent update\n",
    "        state_batch = np.array(state_batch)\n",
    "\n",
    "        loss = self.pi_v_network.update(state_batch, action_batch, target_batch)\n",
    "        print('\\rINFO - Step {}, rl-loss: {}'.format(self.total_t, loss), end='')\n",
    "\n",
    "        self.train_t += 1\n",
    "\n",
    "        if self.save_path and self.train_t % self.save_every == 0:\n",
    "            # To preserve every checkpoint separately, \n",
    "            # add another argument to the function call parameterized by self.train_t\n",
    "            self.save_checkpoint(self.save_path)\n",
    "            print(\"\\nINFO - Saved model checkpoint.\")\n",
    "\n",
    "\n",
    "    def feed_memory(self, state, action, reward, next_state, legal_actions, done):\n",
    "        ''' Feed transition to memory\n",
    "\n",
    "        Args:\n",
    "            state (numpy.array): the current state\n",
    "            action (int): the performed action ID\n",
    "            reward (float): the reward received\n",
    "            next_state (numpy.array): the next state after performing the action\n",
    "            legal_actions (list): the legal actions of the next state\n",
    "            done (boolean): whether the episode is finished\n",
    "        '''\n",
    "        self.memory.save(state, action, reward, next_state, legal_actions, done)\n",
    "\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        self.pi_v_network.device = device\n",
    "\n",
    "    def checkpoint_attributes(self):\n",
    "        '''\n",
    "        Return the current checkpoint attributes (dict)\n",
    "        Checkpoint attributes are used to save and restore the model in the middle of training\n",
    "        Saves the model state dict, optimizer state dict, and all other instance variables\n",
    "        '''\n",
    "        \n",
    "        return {\n",
    "            'agent_type': 'DQNAgent',\n",
    "            'q_estimator': self.q_estimator.checkpoint_attributes(),\n",
    "            'memory': self.memory.checkpoint_attributes(),\n",
    "            'total_t': self.total_t,\n",
    "            'train_t': self.train_t,\n",
    "            'replay_memory_init_size': self.replay_memory_init_size,\n",
    "            'update_target_estimator_every': self.update_target_estimator_every,\n",
    "            'discount_factor': self.discount_factor,\n",
    "            'epsilon_start': self.epsilons.min(),\n",
    "            'epsilon_end': self.epsilons.max(),\n",
    "            'epsilon_decay_steps': self.epsilon_decay_steps,\n",
    "            'batch_size': self.batch_size,\n",
    "            'num_actions': self.num_actions,\n",
    "            'train_every': self.train_every,\n",
    "            'device': self.device,\n",
    "            'save_path': self.save_path,\n",
    "            'save_every': self.save_every\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint):\n",
    "        '''\n",
    "        Restore the model from a checkpoint\n",
    "        \n",
    "        Args:\n",
    "            checkpoint (dict): the checkpoint attributes generated by checkpoint_attributes()\n",
    "        '''\n",
    "        \n",
    "        print(\"\\nINFO - Restoring model from checkpoint...\")\n",
    "        agent_instance = cls(\n",
    "            replay_memory_size=checkpoint['memory']['memory_size'],\n",
    "            replay_memory_init_size=checkpoint['replay_memory_init_size'],\n",
    "            update_target_estimator_every=checkpoint['update_target_estimator_every'],\n",
    "            discount_factor=checkpoint['discount_factor'],\n",
    "            epsilon_start=checkpoint['epsilon_start'],\n",
    "            epsilon_end=checkpoint['epsilon_end'],\n",
    "            epsilon_decay_steps=checkpoint['epsilon_decay_steps'],\n",
    "            batch_size=checkpoint['batch_size'],\n",
    "            num_actions=checkpoint['num_actions'], \n",
    "            state_shape=checkpoint['q_estimator']['state_shape'],\n",
    "            train_every=checkpoint['train_every'],\n",
    "            mlp_layers=checkpoint['q_estimator']['mlp_layers'],\n",
    "            learning_rate=checkpoint['q_estimator']['learning_rate'],\n",
    "            device=checkpoint['device'],\n",
    "            save_path=checkpoint['save_path'],\n",
    "            save_every=checkpoint['save_every'],\n",
    "        )\n",
    "        \n",
    "        agent_instance.total_t = checkpoint['total_t']\n",
    "        agent_instance.train_t = checkpoint['train_t']\n",
    "        \n",
    "        agent_instance.q_estimator = Estimator.from_checkpoint(checkpoint['q_estimator'])\n",
    "        agent_instance.target_estimator = deepcopy(agent_instance.q_estimator)\n",
    "        agent_instance.memory = Memory.from_checkpoint(checkpoint['memory'])\n",
    "\n",
    "        return agent_instance\n",
    "                     \n",
    "    def save_checkpoint(self, path, filename='checkpoint_dqn.pt'):\n",
    "        ''' Save the model checkpoint (all attributes)\n",
    "\n",
    "        Args:\n",
    "            path (str): the path to save the model\n",
    "            filename(str): the file name of checkpoint\n",
    "        '''\n",
    "        torch.save(self.checkpoint_attributes(), os.path.join(path, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, parent=None, parent_action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.children = []\n",
    "        self._number_of_visits = 0\n",
    "        self._results = defaultdict(int)\n",
    "        self._results[1] = 0\n",
    "        self._results[-1] = 0\n",
    "        self._untried_actions = None\n",
    "        self._untried_actions = self.untried_actions()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarloTreeSearch:\n",
    "    def __init__(self, c: float = math.sqrt(2)):\n",
    "        self.c = c # Controls the degree of exploration \n",
    "        self.visited = set()\n",
    "        self.P = defaultdict(list) # s -> policy at state s\n",
    "        self.Q = defaultdict(list)\n",
    "        self.N = defaultdict(list)\n",
    "\n",
    "    def upper_confidence_bound(self, state, action):\n",
    "        return (\n",
    "            self.Q[state][action] + \n",
    "            self.c *\n",
    "            math.sqrt(math.log(sum(self.N[state])) / self.N[state][action])\n",
    "        )\n",
    "\n",
    "    def search(self, state, game, agent):\n",
    "        if game.game_ended(state): \n",
    "            return -game.game_reward(state)\n",
    "\n",
    "        if state not in self.visited:\n",
    "            self.visited.add(state)\n",
    "            self.P[state], v = agent.predict(state)\n",
    "            return -v\n",
    "    \n",
    "        max_u, best_action = -float(\"inf\"), -1\n",
    "        for action in game.get_valid_actions(state):\n",
    "            u = self.upper_confidence_bound(state, action)\n",
    "            if u > max_u:\n",
    "                max_u = u\n",
    "                best_action = action\n",
    "        \n",
    "        next_state = game.next_state(state, best_action)\n",
    "        v = self.search(next_state, game, agent)\n",
    "\n",
    "        self.Q[state][best_action] = (N[s][a] * Q[s][a] + v) / (N[s][a] + 1)\n",
    "        self.N[state][best_action] += 1\n",
    "        return -v\n",
    "\n",
    "def monte_carlo_tree_search(state, game, nnet, c: float = 1.0):\n",
    "    visited = set()\n",
    "    P = defaultdict(list) # s -> policy at state s\n",
    "    Q = defaultdict(list) # s -> Q-value at state s\n",
    "    N = defaultdict(list) # s -> count of times action taken at state s\n",
    "    def search(s, game, nnet):\n",
    "        \"\"\"\n",
    "        Monte-Carlo Tree Search subroutine.\n",
    "        \"\"\"\n",
    "        if game.gameEnded(s): return -game.gameReward(s)\n",
    "\n",
    "        if s not in visited:\n",
    "            visited.add(s)\n",
    "            P[s], v = nnet.predict(s)\n",
    "            return -v\n",
    "    \n",
    "        max_u, best_a = -float(\"inf\"), -1\n",
    "        for a in game.getValidActions(s):\n",
    "            u = Q[s][a] + c_puct*P[s][a]*sqrt(sum(N[s]))/(1+N[s][a])\n",
    "            if u>max_u:\n",
    "                max_u = u\n",
    "                best_a = a\n",
    "        a = best_a\n",
    "        \n",
    "        sp = game.nextState(s, a)\n",
    "        v = search(sp, game, nnet)\n",
    "\n",
    "        Q[s][a] = (N[s][a]*Q[s][a] + v)/(N[s][a]+1)\n",
    "        N[s][a] += 1\n",
    "        return -v\n",
    "\n",
    "\n",
    "    for i in range(1000):\n",
    "        search(state, game, nnet)\n",
    "    return max(N[state], key=N[state].get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' An example of training a reinforcement learning agent on the environments in RLCard\n",
    "'''\n",
    "\n",
    "def prepare_environment(args):\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "\n",
    "    # Seed numpy, torch, random\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(args.env, config={'seed': args.seed})\n",
    "\n",
    "    # Initialize the agent and use random agents as opponents\n",
    "    if args.algorithm == 'dqn':\n",
    "        from rlcard.agents import DQNAgent\n",
    "        if args.load_checkpoint_path != \"\":\n",
    "            agent = DQNAgent.from_checkpoint(checkpoint=torch.load(args.load_checkpoint_path))\n",
    "        else:\n",
    "            agent = DQNAgent(\n",
    "                num_actions=env.num_actions,\n",
    "                state_shape=env.state_shape[0],\n",
    "                mlp_layers= args.mlp_layers,\n",
    "                device=device,\n",
    "                save_path=args.log_dir,\n",
    "                save_every=args.save_every,\n",
    "                estimator_network=args.estimator_network,\n",
    "            )\n",
    "    elif args.algorithm == 'nfsp':\n",
    "        from rlcard.agents import NFSPAgent\n",
    "        if args.load_checkpoint_path != \"\":\n",
    "            agent = NFSPAgent.from_checkpoint(checkpoint=torch.load(args.load_checkpoint_path))\n",
    "        else:\n",
    "            agent = NFSPAgent(\n",
    "                num_actions=env.num_actions,\n",
    "                state_shape=env.state_shape[0],\n",
    "                hidden_layers_sizes=args.hidden_layers_sizes,\n",
    "                q_mlp_layers=[64,64],\n",
    "                device=device,\n",
    "                save_path=args.log_dir,\n",
    "                save_every=args.save_every,\n",
    "                estimator_network=args.estimator_network,\n",
    "            )\n",
    "    elif args.algorithm == 'cfr':\n",
    "        from rlcard.agents import CFRAgent\n",
    "        if args.load_checkpoint_path != \"\":\n",
    "            agent = CFRAgent.from_checkpoint(checkpoint=torch.load(args.load_checkpoint_path))\n",
    "        else:\n",
    "            agent = CFRAgent(\n",
    "                env,\n",
    "                #device=device,\n",
    "                model_path=args.log_dir,\n",
    "                #save_every=args.save_every\n",
    "            )\n",
    "    # Set agents to environment\n",
    "    agents = [agent]\n",
    "    for _ in range(1, env.num_players):\n",
    "        agents.append(RandomAgent(num_actions=env.num_actions)) # Random agents as opponents\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    return env, agent, agents\n",
    "\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    env, agent, agents = prepare_environment(args)\n",
    "\n",
    "# Start training\n",
    "    with Logger(args.log_dir) as logger:\n",
    "        for episode in range(args.num_episodes):\n",
    "\n",
    "            if args.algorithm == 'nfsp':\n",
    "                agents[0].sample_episode_policy()\n",
    "\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            if episode % args.evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    episode,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        args.num_eval_games,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, args.algorithm)\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(args.log_dir, 'model.pth')\n",
    "    torch.save(agent, save_path)\n",
    "    print('Model saved in', save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
