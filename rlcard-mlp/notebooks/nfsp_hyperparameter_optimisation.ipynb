{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import os \n",
    "import torch \n",
    "import rlcard \n",
    "from rlcard.envs.leducholdem import LeducholdemEnv\n",
    "from rlcard.agents import RandomAgent, DQNAgent\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# TODO Move to utils\n",
    "def dataclass_to_grid(dc):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of arguments into a list of dictionaries where each dictionary is a unique \n",
    "    combination of arguments.\n",
    "    \"\"\"\n",
    "    d = asdict(dc)\n",
    "    keys = d.keys()\n",
    "    values = (d[key] if isinstance(d[key], list) else [d[key]] for key in keys)\n",
    "    return [dict(zip(keys, combination)) for combination in product(*values)]\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class MLPHyperparams:\n",
    "    mlp_layers: list[int]\n",
    "    replay_memory_size: int\n",
    "    discount_factor: float\n",
    "    learning_rate: float\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f\"mlp_layers={self.mlp_layers}_replay_memory_size={self.replay_memory_size}_discount_factor={self.discount_factor}_learning_rate={self.learning_rate}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLPHyperparamsGrid:\n",
    "    mlp_layers: list[list[int]] = field(default_factory=lambda: [[128, 128]]) # [512, 1024, 2048, 1024, 512]])\n",
    "    replay_memory_size: list[int] = field(default_factory=lambda: [2000, 100000])\n",
    "    discount_factor: float = field(default=0.99)\n",
    "    learning_rate: float = field(default=0.00005)\n",
    "\n",
    "    def to_hyperparams(self) -> list[MLPHyperparams]:\n",
    "        return [MLPHyperparams(**d) for d in dataclass_to_grid(self)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlcard.agents import NFSPAgent\n",
    "\n",
    "\n",
    "def create_env(seed: int = 0) -> LeducholdemEnv:\n",
    "    \"\"\"\n",
    "    Creates the Leduc Hold'em environment, setting the selected random seed.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    return rlcard.make('leduc-holdem', config={'seed': seed})\n",
    "\n",
    "\n",
    "def create_agent(\n",
    "    env: LeducholdemEnv, \n",
    "    load_checkpoint_path: str = \"\", \n",
    "    device=None, \n",
    "    log_dir: str = './', \n",
    "    save_every: int = 1000, \n",
    "    mlp_layers: list[int] = [128, 128], \n",
    "    replay_memory_size: int = 20000,\n",
    "    learning_rate: float = 0.00005,\n",
    "    discount_factor: float = 0.99,\n",
    "    experiment_name: str = \"\",\n",
    "    agent: str = \"dqn\"\n",
    ") -> DQNAgent:\n",
    "    device = device or get_device()\n",
    "    if agent == \"dqn\":\n",
    "        agent_class = DQNAgent\n",
    "    else:\n",
    "        agent_class = NFSPAgent\n",
    "    if load_checkpoint_path != \"\":\n",
    "        agent = agent_class.from_checkpoint(checkpoint=torch.load(load_checkpoint_path))\n",
    "    else:\n",
    "        agent = agent_class(\n",
    "            num_actions=env.num_actions,\n",
    "            state_shape=env.state_shape[0],\n",
    "            device=device,\n",
    "            save_path=os.path.join(log_dir, experiment_name),\n",
    "            save_every=save_every,\n",
    "            estimator_network='mlp',\n",
    "            mlp_layers=mlp_layers.copy(),\n",
    "            replay_memory_size=replay_memory_size,\n",
    "            learning_rate=learning_rate,\n",
    "            discount_factor=discount_factor,\n",
    "        )\n",
    "    return agent \n",
    "\n",
    "\n",
    "def fill_env_with_agents(env: LeducholdemEnv, agents: list[DQNAgent]) -> None:\n",
    "    \"\"\"\n",
    "    Fills the environment with the given agents and possibly random agents.\n",
    "    \"\"\"\n",
    "    for _ in range(len(agents), env.num_players):\n",
    "        agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "    env.set_agents(agents)\n",
    "\n",
    "\n",
    "def train(env: LeducholdemEnv, agent: DQNAgent, num_steps: int, num_eval_games: int = 100, evaluate_every: int = 100):\n",
    "    fill_env_with_agents(env, [agent])\n",
    "    \n",
    "    # Start training\n",
    "    with Logger(agent.save_path) as logger:\n",
    "        episode = 0\n",
    "        while agent.total_t <= num_steps:\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            if episode % evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    episode,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        num_eval_games,\n",
    "                    )[0]\n",
    "                )\n",
    "            episode += 1\n",
    "\n",
    "    # Get the paths\n",
    "    csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, 'DQN')\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(agent.save_path, 'model.pth')\n",
    "    torch.save(agent, save_path)\n",
    "    print('Model saved in', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hyperparams: MLPHyperparams, num_steps: int = 25000, num_eval_games: int = 2000, seed: int = 0, evaluate_every: int = 100, agent: str = \"dqn\"):\n",
    "    env = create_env(seed)\n",
    "    agent = create_agent(\n",
    "        env, mlp_layers=hyperparams.mlp_layers, \n",
    "        replay_memory_size=hyperparams.replay_memory_size, \n",
    "        learning_rate=hyperparams.learning_rate,\n",
    "        discount_factor=hyperparams.discount_factor, \n",
    "        experiment_name=hyperparams.name,\n",
    "        log_dir=f'./experiments/{seed}/',\n",
    "        agent=agent,\n",
    "    )\n",
    "    train(env, agent, num_steps=num_steps, num_eval_games=num_eval_games, evaluate_every=evaluate_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running on the CPU\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DQNAgent.__init__() got multiple values for argument 'replay_memory_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hyperparams \u001b[38;5;129;01min\u001b[39;00m MLPHyperparamsGrid()\u001b[38;5;241m.\u001b[39mto_hyperparams():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnfsp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(hyperparams, num_steps, num_eval_games, seed, evaluate_every, agent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(hyperparams: MLPHyperparams, num_steps: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25000\u001b[39m, num_eval_games: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, evaluate_every: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, agent: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     env \u001b[38;5;241m=\u001b[39m create_env(seed)\n\u001b[0;32m----> 3\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_memory_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_memory_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./experiments/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     train(env, agent, num_steps\u001b[38;5;241m=\u001b[39mnum_steps, num_eval_games\u001b[38;5;241m=\u001b[39mnum_eval_games, evaluate_every\u001b[38;5;241m=\u001b[39mevaluate_every)\n",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m, in \u001b[0;36mcreate_agent\u001b[0;34m(env, load_checkpoint_path, device, log_dir, save_every, mlp_layers, replay_memory_size, learning_rate, discount_factor, experiment_name, agent)\u001b[0m\n\u001b[1;32m     31\u001b[0m     agent \u001b[38;5;241m=\u001b[39m agent_class\u001b[38;5;241m.\u001b[39mfrom_checkpoint(checkpoint\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload(load_checkpoint_path))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43magent_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmlp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_layers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_memory_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_memory_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent\n",
      "File \u001b[0;32m~/mlpractical-assignment4/rlcard-mlp/notebooks/../rlcard/agents/nfsp_agent.py:127\u001b[0m, in \u001b[0;36mNFSPAgent.__init__\u001b[0;34m(self, num_actions, state_shape, hidden_layers_sizes, reservoir_buffer_capacity, anticipatory_param, batch_size, train_every, rl_learning_rate, sl_learning_rate, min_buffer_size_to_learn, q_replay_memory_size, q_replay_memory_init_size, q_update_target_estimator_every, q_discount_factor, q_epsilon_start, q_epsilon_end, q_epsilon_decay_steps, q_batch_size, q_train_every, q_mlp_layers, evaluate_with, device, save_path, save_every, estimator_network, mlp_layers, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Build the action-value network\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rl_agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_replay_memory_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_replay_memory_init_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_update_target_estimator_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_discount_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_epsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_epsilon_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_epsilon_decay_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_train_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_mlp_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrl_learning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Build the average policy supervised model\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_model()\n",
      "\u001b[0;31mTypeError\u001b[0m: DQNAgent.__init__() got multiple values for argument 'replay_memory_size'"
     ]
    }
   ],
   "source": [
    "for hyperparams in MLPHyperparamsGrid().to_hyperparams():\n",
    "    run(hyperparams, num_steps=250000, seed=2, evaluate_every=100, agent='nfsp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgp_requirements_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
